{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1438364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.19 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "900bcfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4767ff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "240d476d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11d14412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.04500000085681677\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: 0.04500000085681677\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        #print(rewards) # (1,2)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c9c3eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim,lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self,state_size,action_size,seed,fc1_units=256,fc2_units=128):\n",
    "        super(Actor,self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.num_agents = 2\n",
    "        self.fc1 = nn.Linear(state_size,fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units,fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units,action_size)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3,3e-3)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "    \n",
    "    \n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self,state_size,action_size,seed,fc1_units=256,fc2_units=128):\n",
    "        super(Critic,self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.num_agents = 2\n",
    "        self.fc1 = nn.Linear(state_size*self.num_agents,fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units+(action_size*self.num_agents),fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units,1)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3,3e-3)\n",
    "        \n",
    "    def forward(self,state,action):\n",
    "        xs = F.relu(self.fc1(state))\n",
    "        x = torch.cat((xs,action),dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f1bb5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)\n",
    "BATCH_SIZE = 128 #128\n",
    "GAMMA = 0.99\n",
    "TAU = 7e-2\n",
    "LR_ACTOR = 1e-3\n",
    "LR_CRITIC = 1e-4\n",
    "WEIGHT_DECAY = 0\n",
    "EPSILON = 5.5\n",
    "EPSILON_DECAY = 1e-4\n",
    "EPSILON_FINAL = 0.001\n",
    "MULTIPLE_LEARN_PER_UPDATE = 5\n",
    "LEARN_EVERY = 1\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb335406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "class OUNoise:\n",
    "    def __init__(self,size,seed,mu=0.,theta=0.15,sigma=0.2):\n",
    "        self.size = size\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = copy.copy(self.mu)\n",
    "        \n",
    "    def sample(self):\n",
    "        x = self.state\n",
    "        #dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22344900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple,deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self,action_size,buffer_size,batch_size,seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\",field_names=[\"state\",\"action\",\"reward\",\"next_state\",\"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.num_agents = 2\n",
    "        \n",
    "    def add(self,state,action,reward,next_state,done):\n",
    "        e = self.experience(state,action,reward,next_state,done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory,k=self.batch_size)\n",
    "        states = [torch.from_numpy(np.vstack([e.state[index] for e in experiences if e is not None])).float().to(device) for index in range(self.num_agents)]\n",
    "        actions = [torch.from_numpy(np.vstack([e.action[index] for e in experiences if e is not None])).float().to(device) for index in range(self.num_agents)]\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = [torch.from_numpy(np.vstack([e.next_state[index] for e in experiences if e is not None])).float().to(device) for index in range(self.num_agents)]\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states,actions,rewards,next_states,dones)\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "863ce2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class ddpg_agent():\n",
    "    def __init__(self,state_size,action_size,num_agents,random_seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = num_agents\n",
    "        self.seed = random.seed(random_seed)\n",
    "        \n",
    "        self.actor_local = Actor(state_size,action_size,random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size,action_size,random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(),lr=LR_ACTOR)\n",
    "        \n",
    "        self.critic_local = Critic(state_size,action_size,random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size,action_size,random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(),lr=LR_CRITIC,weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        self.noise = OUNoise(action_size,random_seed)\n",
    "        \n",
    "        self.step_t = 0\n",
    "        self.epsilon = EPSILON\n",
    "        \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "        \n",
    "        \n",
    "    def step(self,buffer,agent_number):\n",
    "        self.step_t = (self.step_t + 1) % LEARN_EVERY\n",
    "        \n",
    "        if len(buffer) > BATCH_SIZE:\n",
    "            if self.step_t == 0:\n",
    "                experiences = buffer.sample()\n",
    "                self.learn(experiences,GAMMA,agent_number)\n",
    "                \n",
    "    def act(self,state,add_noise=True):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "            \n",
    "        self.actor_local.train()\n",
    "        \n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "            \n",
    "        return np.clip(action, -1, 1)\n",
    "    \n",
    "    def learn(self,experiences,gamma,agent_number):\n",
    "        states_list,actions_list,rewards,next_states_list,dones = experiences\n",
    "        \n",
    "        next_states_tensor = torch.cat(next_states_list,dim=1).to(device)\n",
    "        states_tensor = torch.cat(states_list,dim=1).to(device)\n",
    "        actions_tensor = torch.cat(actions_list,dim=1).to(device)\n",
    "        \n",
    "        next_actions = [self.actor_target(states) for states in states_list]\n",
    "        next_actions_tensor = torch.cat(next_actions,dim=1).to(device)\n",
    "        \n",
    "        Q_targets_next = self.critic_target(next_states_tensor,next_actions_tensor)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        Q_expected = self.critic_local(states_tensor,actions_tensor)\n",
    "        #critic_loss = F.mse_loss(Q_expected,Q_targets[:,agent_number].reshape(-1,1))  Don't use reshape causes the below error\n",
    "        #RuntimeError: Tensor: invalid storage offset at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/THC/generic/THCTensor.c:759\n",
    "        critic_loss = F.mse_loss(Q_expected,Q_targets[:,agent_number].view(-1,1))\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        \n",
    "        #Actor\n",
    "        actions_pred = [self.actor_local(states) for states in states_list]\n",
    "        actions_pred_tensor = torch.cat(actions_pred,dim=1).to(device)\n",
    "        \n",
    "        actor_loss = -self.critic_local(states_tensor,actions_pred_tensor).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        self.soft_update(self.critic_local,self.critic_target,TAU)\n",
    "        self.soft_update(self.actor_local,self.actor_target,TAU)\n",
    "        \n",
    "    def soft_update(self,local_model,target_model,tau): \n",
    "        for target_param,local_param in zip(target_model.parameters(),local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a875526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPG():\n",
    "    def __init__(self,state_size,action_size,num_agents,random_seed):\n",
    "        self.agents = [ddpg_agent(state_size,action_size,num_agents,random_seed) for x in range(num_agents)]\n",
    "        self.memory = ReplayBuffer(action_size,BUFFER_SIZE,BATCH_SIZE,random_seed)\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = num_agents\n",
    "        \n",
    "    def step(self,states,actions,next_states,rewards,dones):\n",
    "        self.memory.add(states,actions,rewards,next_states,dones)\n",
    "    \n",
    "        for index,agent in enumerate(self.agents):\n",
    "            agent.step(self.memory,index)\n",
    "            \n",
    "    def act(self,states,add_noise=True):\n",
    "        actions = np.zeros([self.num_agents,self.action_size])\n",
    "        \n",
    "        for index,agent in enumerate(self.agents):\n",
    "            actions[index,:] = agent.act(states[index],add_noise)\n",
    "            \n",
    "        return actions\n",
    "    \n",
    "    def checkpoint(self):\n",
    "        for index,agent in enumerate(self.agents):\n",
    "            torch.save(agent.actor_local.state_dict(), 'agent{}_checkpoint_actor_final.pth'.format(index))\n",
    "            torch.save(agent.critic_local.state_dict(), 'agent{}_checkpoint_critic_final.pth'.format(index))\n",
    "            \n",
    "    def reset(self):\n",
    "        for agent in self.agents:\n",
    "            agent.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87573bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "maddpg_model = MADDPG(state_size,action_size,num_agents,random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1e9ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from workspace_utils import keep_awake\n",
    "def train_maddpg(n_episodes=5000,print_every=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    average_scores_list = []\n",
    "    \n",
    "    for i_episode in keep_awake(range(n_episodes + 1)):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        score = np.zeros(num_agents)\n",
    "        maddpg_model.reset()\n",
    "        \n",
    "        while True:\n",
    "            actions = maddpg_model.act(states)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            maddpg_model.step(states,actions,next_states,rewards,dones)\n",
    "            states = next_states\n",
    "            score += rewards\n",
    "            \n",
    "            if any(dones):\n",
    "                break\n",
    "                \n",
    "        score_max = np.max(score)\n",
    "        scores.append(score_max)\n",
    "        scores_deque.append(score_max)\n",
    "        average_score = np.mean(scores_deque)\n",
    "        average_scores_list.append(average_score)\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.4f}'.format(i_episode, np.mean(scores_deque)), end=\"\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.4f}'.format(i_episode, average_score))\n",
    "            \n",
    "            if average_score >= 0.5:\n",
    "                print('\\rEnvironment solved in {} episodes with an Average Score of {:.4f}'.format(i_episode, average_score))\n",
    "                maddpg_model.checkpoint()\n",
    "                return scores,average_scores_list\n",
    "            #print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, avg_score))\n",
    "            \n",
    "    return scores,average_scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba07ef5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tAverage Score: 0.0000\n",
      "Episode 100\tAverage Score: 0.0000\n",
      "Episode 200\tAverage Score: 0.0009\n",
      "Episode 300\tAverage Score: 0.0009\n",
      "Episode 400\tAverage Score: 0.0099\n",
      "Episode 500\tAverage Score: 0.0069\n",
      "Episode 600\tAverage Score: 0.0391\n",
      "Episode 700\tAverage Score: 0.0765\n",
      "Episode 800\tAverage Score: 0.0827\n",
      "Episode 900\tAverage Score: 0.0996\n",
      "Episode 1000\tAverage Score: 0.1040\n",
      "Episode 1100\tAverage Score: 0.1057\n",
      "Episode 1200\tAverage Score: 0.1174\n",
      "Episode 1300\tAverage Score: 0.1817\n",
      "Episode 1400\tAverage Score: 0.2356\n",
      "Episode 1500\tAverage Score: 0.2752\n",
      "Episode 1600\tAverage Score: 0.3439\n",
      "Episode 1700\tAverage Score: 0.4254\n",
      "Episode 1800\tAverage Score: 0.4585\n",
      "Episode 1900\tAverage Score: 0.5886\n",
      "Environment solved in 1900 episodes with an Average Score of 0.5886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f36da8a5f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "scores,average_scores_list = train_maddpg()\n",
    "plt.plot(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b037e576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3646d9c550>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a56f2228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
